# x-airflow-common:
#   &airflow-common
#     build:
#       context: ./airflow_scheduler_service
#       dockerfile: Dockerfile

#     image: airflow:latest
#     environment:
#       &airflow-common-env
#       AIRFLOW__CORE__EXECUTOR: LocalExecutor
#       AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
#       AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
#       AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
#       AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
#       AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
#       environment:
#       AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_ENABLED: 'True'
#       AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_SIZE: 5
#       AIRFLOW__DATABASE__SQL_ALCHEMY_MAX_OVERFLOW: 10
#       AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_RECYCLE: 1800
#     volumes:
#       - ./airflow_scheduler_service/dags:/opt/airflow/dags
#       - ./logs:/opt/airflow/logs
#     user: "${AIRFLOW_UID:-50000}:0"
#     depends_on:
#       kafka:
#         condition: service_started
services:
  # airflow:
  #   <<: *airflow-common
  #   ports:
  #     - "8080:8080"
  #   healthcheck:
  #     test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
  #     interval: 10s
  #     timeout: 10s
  #     retries: 5
  #   entrypoint: ["/entrypoint.sh"]
  #   networks:
  #     - stock_pipeline

  # load-balancer:
  #   image: nginx:latest
  #   volumes:
  #     - ./nginx.conf:/etc/nginx/nginx.conf:ro
  #   ports:
  #     - "80:80"
  #   depends_on:
  #     - data-collector
  #   networks:
  #     - stock_pipeline
  
  # postgres:
  #   image: postgres:15
  #   environment:
  #     - POSTGRES_USER=sa
  #     - POSTGRES_PASSWORD=Passw0rd!
  #     - POSTGRES_DB=us_stock_options_db
  #   ports:
  #     - "5432:5432"
  #   # sn/bash", "/postgres-entrypoint.sh"]
  #   volumes:
  #     - postgres_data:/var/lib/postgresql/data
  #     - ./init-db.sql:/docker-entrypoint-initdb.d/init-db.sql
  #     - ./postgres-entrypoint.sh:/usr/local/bin/postgres-entrypoint.sh
  #     - ./backups:/var/backups
  #   entrypoint: ["/usr/local/bin/postgres-entrypoint.sh"]
  #   healthcheck:
  #     test: ["CMD-SHELL", "pg_isready -U sa -d us_stock_options_db"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 10
  #     start_period: 30s
  #   networks:
  #     - stock_pipeline
  #   # command: >
  #   #   bash -c "/postgres-entrypoint.sh"
  
  # pgadmin:
  #   image: dpage/pgadmin4:8.10
  #   environment:
  #     - PGADMIN_DEFAULT_EMAIL=admin@example.com
  #     - PGADMIN_DEFAULT_PASSWORD=admin
  #     - PGADMIN_CONFIG_SERVER_MODE=True
  #     - PGADMIN_CONFIG_MASTER_PASSWORD_REQUIRED=False
  #   ports:
  #     - "5050:80"
  #   depends_on:
  #     postgres:
  #       condition: service_healthy
  #   networks:
  #     - stock_pipeline
  #   volumes:
  #     - pgadmin_data:/var/lib/pgadmin
  #   restart: unless-stopped

  data-collector:
    build:
      context: ./data_collector_service
      dockerfile: Dockerfile
      

    image: data-collector:latest

    environment:
      # - TWELVE_DATA_API_1=889ef64f57ef4abea88ae2e0bbbe3537
      # - TWELVE_DATA_API_2=e4070b64b1aa414db52a8350e38b0367
      # - TWELVE_DATA_API_3=200f5a7b095d40ab84753210bc4a95fa
      # - TWELVE_DATA_API_4=99af927d7def48d3b37a8ee98b7e090b
      # - TWELVE_DATA_API_5=8c0c5c94152c4353a2c3afe5704ea840
      - TWELVE_DATA_API_1=f4de2d1297354c6b81a4b0d017fc6d06
      - TWELVE_DATA_API_2=62e7d72673964d799b868de8a9e7283b
      - TWELVE_DATA_API_3=ab27dda112bd4fac8721226498d8d59a
      - TWELVE_DATA_API_4=403631c4756e4d00b52455514acd8124
      - TWELVE_DATA_API_5=ece1bbcef7a341beb1629adabc723fdd
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_TRIGGER_DAILY=fetch-trigger-daily
      - KAFKA_TRIGGER_15MIN=fetch-trigger-15min
      - KAFKA_TRIGGER_HISTORICAL=fetch-trigger-historical
      - KAFKA_DAILY_TOPIC=daily-data
      - KAFKA_15min_TOPIC=15min-data
      - KAFKA_OPTION_TOPIC=options-data
      - KAFKA_HISTORICAL_TOPIC=historical-data
      - KAFKA_TASK_TOPIC=task-queue
      - DJANGO_SECRET_KEY="django-insecure-k^c404!2*woj(h(+ek*e#=0sh^qrjw9y-g34m^*qy=^=!43v%^"
      - APP_DEBUG=True
    ports:
      - "8000:8000"
    depends_on:
      - kafka-service
    networks:
      - stock_pipeline
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 4G
        reservations:
          cpus: '0.5'
          memory: 2G
  
  data-processor:
    build:
      context: ./data_processor_service
      dockerfile: Dockerfile
      

    image: data-processor:latest

    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_DAILY_TOPIC=daily-data
      - KAFKA_15min_TOPIC=15min-data
      - KAFKA_OPTION_TOPIC=options-data
      - KAFKA_HISTORICAL_TOPIC=historical-data
      - KAFKA_PROCESSED_DAILY=processed-daily-data
      - KAFKA_PROCESSED_15MIN=processed-15min-data
      - KAFKA_PROCESSED_OPTIONS=processed-options-data
      - KAFKA_PROCESSED_HISTORICAL=processed-historical-data
      - KAFKA_PROCESSED_FILE_DAILY=processed-file-daily-data
      - KAFKA_PROCESSED_FILE_15MIN=processed-file-15min-data
      - KAFKA_PROCESSED_FILE_OPTIONS=processed-file-options-data
      - KAFKA_PROCESSED_FILE_HISTORICAL=processed-file-historical-data

      - DJANGO_SECRET_KEY=django-insecure-#2k^ti5^bcw_ce5-73!ce&w6ywg)6(_9gpceh+%!nqa^yp1=r6
      - APP_DEBUG=True
    ports:
      - "8001:8001"
    depends_on:
      - kafka-service
    networks:
      - stock_pipeline

  database-writer:
    build:
      context: ./database_writer_service
      dockerfile: Dockerfile
      

    image: database-writer:latest

    environment:
      - AZURE_SQL_CONNECTION_STRING=${AZURE_SQL_CONNECTION_STRING}
      - POSTGRES_CONNECTION_STRING=postgresql://sa:Passw0rd!@postgres:5432/us_stock_options_db
      - INFLUXDB_URL=https://us-east-1-1.aws.cloud2.influxdata.com
      - INFLUXDB_TOKEN=p4vPDiJyynco8tjaNhG2ch7A51SHtzN0ta3VsJ6Y1OqVyHtvuAL7K_gKOVmsYV47F_hqaNPlHKOdi6Y_C8Xjw==
      - INFLUXDB_ORG=US_Stock_Data
      - INFLUXDB_BUCKET=fifteenmin_stockdata

      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_PROCESSED_DAILY=processed-daily-data
      - KAFKA_PROCESSED_15MIN=processed-15min-data
      - KAFKA_PROCESSED_OPTIONS=processed-options-data
      - KAFKA_PROCESSED_HISTORICAL=processed-historical-data
      
      - DJANGO_SECRET_KEY=django-insecure-np#5k+3!h2y5he0r=inoh*l5^nw*!t-&e^t-p5zb5ror*^jkyw
      - APP_DEBUG=True
      - RUN_DB_HANDLER=True
    
    volumes:
      - ./backups:/var/backups
    ports:
      - "8002:8002"
    depends_on:
      - kafka-service
    networks:
      - stock_pipeline

  file-writer:
    build:
      context: ./file_writer_service    
      dockerfile: Dockerfile
      

    image: file-writer:latest

    environment:
      - AZURE_SQL_CONNECTION_STRING={ODBC Driver 18 for SQL Server};Server=tcp:dash-gtd02.database.windows.net,1433;Database=us_stock_options_db;Uid=dash_gtd;Pwd=wearethebest@69;Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;
      - POSTGRES_CONNECTION_STRING=postgresql://sa:Passw0rd!@postgres:5432/us_stock_options_db
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - S3_BUCKET_NAME=dash-gtd-us-east-1-processed-data
      - INFLUXDB_URL=https://us-east-1-1.aws.cloud2.influxdata.com
      - INFLUXDB_TOKEN=-p4vPDiJyynco8tjaNhG2ch7A51SHtzN0ta3VsJ6Y1OqVyHtvuAL7K_gKOVmsYV47F_hqaNPlHKOdi6Y_C8Xjw==
      - INFLUXDB_ORG=US_Stock_Data
      - INFLUXDB_BUCKET=fifteenmin_stockdata
    ports:
      - "8003:8003"
    # depends_on:
    #   postgres:
    #     condition: service_healthy
    networks:
      - stock_pipeline

  
  data-api-service:
    build:
      context: ./data_api_service
      dockerfile: Dockerfile
      

    image: data-api-service:latest

    environment:
      - AZURE_SQL_CONNECTION_STRING={ODBC Driver 18 for SQL Server};Server=tcp:dash-gtd02.database.windows.net,1433;Database=us_stock_options_db;Uid=dash_gtd;Pwd=wearethebest@69;Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;

      - POSTGRES_CONNECTION_STRING=postgresql://sa:Passw0rd!@postgres:5432/us_stock_options_db
      - SQL_SERVER=dash-gtd.database.windows.net
      - SQL_DATABASE=us_stock_options_db
      - SQL_USERNAME=dash_gtd
      - SQL_PASSWORD=wearethebest@69
      - SQL_DRIVER=ODBC Driver 18 for SQL Server
      - DJANGO_DEBUG=True
      - DJANGO_SECRET_KEY=django-insecure-k^c404!2*woj(h(+ek*e#=0sh^qrjw9y-g34m^*qy=^=!43v%^
      - INFLUXDB_URL=https://us-east-1-1.aws.cloud2.influxdata.com
      - INFLUXDB_TOKEN=-p4vPDiJyynco8tjaNhG2ch7A51SHtzN0ta3VsJ6Y1OqVyHtvuAL7K_gKOVmsYV47F_hqaNPlHKOdi6Y_C8Xjw==
      - INFLUXDB_ORG=US_Stock_Data
      - INFLUXDB_BUCKET=fifteenmin_stockdata
    ports:
      - "8006:8006"
    # depends_on:
    #   postgres:
    #     condition: service_healthy
    networks:
      - stock_pipeline
    restart: unless-stopped

  # prediction-service:
  #   build:
  #     context: ./prediction_service
  #     dockerfile: Dockerfile
  #   image: prediction-service:latest
  #   environment:
  #     - DATA_API_URL=http://data_api_service:8006/api/realtime-data/
  #     # - POSTGRES_CONNECTION_STRING=postgresql://sa:Passw0rd!@postgres:5432/us_stock_options_db
  #     - MODEL_PATH=/app/models/transformer_lstm.pth
  #     - SCALER_PATH=/app/models/scaler.npy
  #   ports:
  #     - "8007:8007"
  #   volumes:
  #     - ./models:/app/models
  #     - ./data:/app/data
  #   depends_on:
  #     data-api-service:
  #       condition: service_started
  #     # postgres:
  #     #   condition: service_healthy
  #   networks:
  #     - stock_pipeline
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: '2.0'
  #         memory: 8G
  #       reservations:
  #         cpus: '1.0'
  #         memory: 4G


  kafka-service:
    image: confluentinc/cp-kafka:7.4.0
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-service:9092

    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions.sh", "--bootstrap-server", "kafka-service:9092"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - stock_pipeline

  kafka-init:
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      - kafka-service
    entrypoint: ["/bin/bash", "-c"]
    command: |
      "
      for i in {1..30}; do
        kafka-broker-api-versions --bootstrap-server kafka-service:9092 && break
        echo 'Waiting for Kafka to be ready...'
        sleep 5
      done
      kafka-topics --create --topic task-queue                     --bootstrap-server kafka-service:9092 --replication-factor 1 --partitions 1 
      kafka-topics --create --topic daily-data                     --bootstrap-server kafka-service:9092 --replication-factor 1 --partitions 5
      kafka-topics --create --topic 15min-data                     --bootstrap-server kafka-service:9092 --replication-factor 1 --partitions 5
      kafka-topics --create --topic options-data                   --bootstrap-server kafka-service:9092 --replication-factor 1 --partitions 5
      kafka-topics --create --topic historical-data                --bootstrap-server kafka-service:9092 --replication-factor 1 --partitions 5
      
      kafka-topics --create --topic processed-daily-data           --bootstrap-server kafka-service:9092 --replication-factor 1 --partitions 5
      kafka-topics --create --topic processed-15min-data           --bootstrap-server kafka-service:9092 --replication-factor 1 --partitions 5
      kafka-topics --create --topic processed-options-data         --bootstrap-server kafka-service:9092 --replication-factor 1 --partitions 5
      kafka-topics --create --topic processed-historical-data      --bootstrap-server kafka-service:9092 --replication-factor 1 --partitions 5
      echo 'Kafka topics created!'
      "
    networks:
      - stock_pipeline

      
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      - ZOOKEEPER_CLIENT_PORT=2181
      - ZOOKEEPER_TICK_TIME=2000
    ports:
      - "2181:2181"
    healthcheck:
      test: ["CMD-SHELL", "echo ruok | nc localhost 2181"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - stock_pipeline

  # minio:
  #   image: minio/minio:latest
  #   environment:
  #     - MINIO_ROOT_USER=minioadmin
  #     - MINIO_ROOT_PASSWORD=minioadmin
  #   command: server /data --console-address ":9001"
  #   ports:
  #     - "9000:9000"
  #     - "9001:9001"
  #   volumes:
  #     - minio_data:/data
  #   networks:
  #     - stock_pipeline

  # prometheus:
  #   image: prom/prometheus:v2.54.1
  #   volumes:
  #     - ./prometheus.yml:/etc/prometheus/prometheus.yml
  #   ports:
  #     - "9090:9090"
  #   networks:
  #     - stock_pipeline
    


  # grafana:
  #   image: grafana/grafana:11.2.2
  #   environment:
  #     - GF_SECURITY_ADMIN_USER=admin
  #     - GF_SECURITY_ADMIN_PASSWORD=admin
  #   ports:
  #     - "3000:3000"
  #   volumes:
  #     - grafana_data:/var/lib/grafana
  #   networks:
  #     - stock_pipeline

  
  # node-exporter:
  #   image: prom/node-exporter:latest
  #   container_name: node_exporter
  #   ports:
  #     - "9100:9100"
  #   networks:
  #     - stock_pipeline

volumes:
  mysql_data:
  influxdb_data:
  minio_data:
  grafana_data:
  postgres_data:
  postgres_backups:
  pgadmin_data:
  models:
  data:

networks:
  stock_pipeline:
    driver: bridge