x-airflow-common:
  &airflow-common
    build:
      context: ./airflow_scheduler_service
      dockerfile: Dockerfile

    image: airflow:latest
    environment:
      &airflow-common-env
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
      environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_ENABLED: 'True'
      AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_SIZE: 5
      AIRFLOW__DATABASE__SQL_ALCHEMY_MAX_OVERFLOW: 10
      AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_RECYCLE: 1800
    volumes:
      - ./airflow_scheduler_service/dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
    user: "${AIRFLOW_UID:-50000}:0"
    depends_on:
      kafka:
        condition: service_started
services:
  airflow:
    <<: *airflow-common
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5
    entrypoint: ["/entrypoint.sh"]
    networks:
      - stock_pipeline

  load-balancer:
    image: nginx:latest
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    ports:
      - "80:80"
    depends_on:
      - data-collector
    networks:
      - stock_pipeline
  
  postgres:
    image: postgres:15
    environment:
      - POSTGRES_USER=sa
      - POSTGRES_PASSWORD=Passw0rd!
      - POSTGRES_DB=us_stock_options_db
    ports:
      - "5432:5432"
    # volumes:
    #   - postgres_data:/var/lib/postgresql/data
    #   - ./init-db.sql:/docker-entrypoint-initdb.d/init-db.sql
    #   - ./postgres-backups:/var/backups
    #   - ./postgres-entrypoint.sh:/postgres-entrypoint.sh
    #   - ./backup-db.sh:/backup-db.sh
    #   - ./backup-cron:/etc/cron.d/backup-cron
    # entrypoint: ["/bin/bash", "/postgres-entrypoint.sh"]
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-db.sql:/docker-entrypoint-initdb.d/init-db.sql
      - ./postgres-entrypoint.sh:/usr/local/bin/postgres-entrypoint.sh
      - ./backups:/var/backups
    entrypoint: ["/usr/local/bin/postgres-entrypoint.sh"]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U sa -d us_stock_options_db"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s
    networks:
      - stock_pipeline
    # command: >
    #   bash -c "/postgres-entrypoint.sh"
  
  pgadmin:
    image: dpage/pgadmin4:8.10
    environment:
      - PGADMIN_DEFAULT_EMAIL=admin@example.com
      - PGADMIN_DEFAULT_PASSWORD=admin
      - PGADMIN_CONFIG_SERVER_MODE=True
      - PGADMIN_CONFIG_MASTER_PASSWORD_REQUIRED=False
    ports:
      - "5050:80"
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - stock_pipeline
    volumes:
      - pgadmin_data:/var/lib/pgadmin
    restart: unless-stopped

  data-collector:
    build:
      context: ./data_collector_service
      dockerfile: Dockerfile
      

    image: data-collector:latest

    environment:
      - TWELVE_DATA_API_1=889ef64f57ef4abea88ae2e0bbbe3537
      - TWELVE_DATA_API_2=e4070b64b1aa414db52a8350e38b0367
      - TWELVE_DATA_API_3=200f5a7b095d40ab84753210bc4a95fa
      - TWELVE_DATA_API_4=99af927d7def48d3b37a8ee98b7e090b
      - TWELVE_DATA_API_5=8c0c5c94152c4353a2c3afe5704ea840
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_TRIGGER_DAILY=fetch-trigger-daily
      - KAFKA_TRIGGER_15MIN=fetch-trigger-15min
      - KAFKA_TRIGGER_HISTORICAL=fetch-trigger-historical
      - KAFKA_DAILY_TOPIC=daily-data
      - KAFKA_15min_TOPIC=15min-data
      - KAFKA_OPTION_TOPIC=options-data
      - KAFKA_HISTORICAL_TOPIC=historical-data
      - DJANGO_SECRET_KEY="django-insecure-k^c404!2*woj(h(+ek*e#=0sh^qrjw9y-g34m^*qy=^=!43v%^"
      - APP_DEBUG=True
    ports:
      - "8000:8000"
    depends_on:
      - kafka
    networks:
      - stock_pipeline
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 4G
        reservations:
          cpus: '0.5'
          memory: 2G
  
  data-processor:
    build:
      context: ./data_processor_service
      dockerfile: Dockerfile
      

    image: data-processor:latest

    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_DAILY_TOPIC=daily-data
      - KAFKA_15min_TOPIC=15min-data
      - KAFKA_OPTION_TOPIC=options-data
      - KAFKA_HISTORICAL_TOPIC=historical-data
      - KAFKA_PROCESSED_DAILY=processed-daily-data
      - KAFKA_PROCESSED_15MIN=processed-15min-data
      - KAFKA_PROCESSED_OPTIONS=processed-options-data
      - KAFKA_PROCESSED_HISTORICAL=processed-historical-data
      - KAFKA_PROCESSED_FILE_DAILY=processed-file-daily-data
      - KAFKA_PROCESSED_FILE_15MIN=processed-file-15min-data
      - KAFKA_PROCESSED_FILE_OPTIONS=processed-file-options-data
      - KAFKA_PROCESSED_FILE_HISTORICAL=processed-file-historical-data

      - DJANGO_SECRET_KEY=django-insecure-#2k^ti5^bcw_ce5-73!ce&w6ywg)6(_9gpceh+%!nqa^yp1=r6
      - APP_DEBUG=True
    ports:
      - "8001:8001"
    depends_on:
      - kafka
    networks:
      - stock_pipeline

  database-writer:
    build:
      context: ./database_writer_service
      dockerfile: Dockerfile
      

    image: database-writer:latest

    environment:
      - AZURE_SQL_CONNECTION_STRING=${AZURE_SQL_CONNECTION_STRING}
      - POSTGRES_CONNECTION_STRING=postgresql://sa:Passw0rd!@postgres:5432/us_stock_options_db
      - INFLUXDB_URL=https://us-east-1-1.aws.cloud2.influxdata.com
      - INFLUXDB_TOKEN=p4vPDiJyynco8tjaNhG2ch7A51SHtzN0ta3VsJ6Y1OqVyHtvuAL7K_gKOVmsYV47F_hqaNPlHKOdi6Y_C8Xjw==
      - INFLUXDB_ORG=US_Stock_Data
      - INFLUXDB_BUCKET=fifteenmin_stockdata

      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_PROCESSED_DAILY=processed-daily-data
      - KAFKA_PROCESSED_15MIN=processed-15min-data
      - KAFKA_PROCESSED_OPTIONS=processed-options-data
      - KAFKA_PROCESSED_HISTORICAL=processed-historical-data
      
      - DJANGO_SECRET_KEY=django-insecure-np#5k+3!h2y5he0r=inoh*l5^nw*!t-&e^t-p5zb5ror*^jkyw
      - APP_DEBUG=True
      - RUN_DB_HANDLER=True
    
    volumes:
      - ./backups:/var/backups
    ports:
      - "8002:8002"
    depends_on:
      - postgres
      - kafka
    networks:
      - stock_pipeline

  file-writer:
    build:
      context: ./file_writer_service    
      dockerfile: Dockerfile
      

    image: file-writer:latest

    environment:
      - AZURE_SQL_CONNECTION_STRING=${AZURE_SQL_CONNECTION_STRING}
      - POSTGRES_CONNECTION_STRING=postgresql://sa:Passw0rd!@postgres:5432/us_stock_options_db
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - S3_BUCKET_NAME=dash-gtd-us-east-1-processed-data
      - INFLUXDB_URL=https://us-east-1-1.aws.cloud2.influxdata.com
      - INFLUXDB_TOKEN=-p4vPDiJyynco8tjaNhG2ch7A51SHtzN0ta3VsJ6Y1OqVyHtvuAL7K_gKOVmsYV47F_hqaNPlHKOdi6Y_C8Xjw==
      - INFLUXDB_ORG=US_Stock_Data
      - INFLUXDB_BUCKET=fifteenmin_stockdata
    ports:
      - "8003:8003"
    depends_on:
      postgres:
        condition: service_healthy # <--- This waits for the healthcheck to pass!
    networks:
      - stock_pipeline

  
  data-api-service:
    build:
      context: ./data_api_service
      dockerfile: Dockerfile
      

    image: data-api-service:latest

    environment:
      - POSTGRES_CONNECTION_STRING=postgresql://sa:Passw0rd!@postgres:5432/us_stock_options_db
      - SQL_SERVER=dash-gtd.database.windows.net
      - SQL_DATABASE=us_stock_options_db
      - SQL_USERNAME=dash_gtd
      - SQL_PASSWORD=wearethebest@69
      - SQL_DRIVER=ODBC Driver 18 for SQL Server
      - DJANGO_DEBUG=True
      - DJANGO_SECRET_KEY=django-insecure-k^c404!2*woj(h(+ek*e#=0sh^qrjw9y-g34m^*qy=^=!43v%^
    ports:
      - "8006:8006"
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - stock_pipeline
    restart: unless-stopped


  kafka:
    image: confluentinc/cp-kafka:7.4.0
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092

    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions.sh", "--bootstrap-server", "kafka:9092"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - stock_pipeline

  kafka-init:
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      - kafka
    entrypoint: ["/bin/bash", "-c"]
    command: |
      "
      for i in {1..30}; do
        kafka-broker-api-versions --bootstrap-server kafka:9092 && break
        echo 'Waiting for Kafka to be ready...'
        sleep 5
      done

      kafka-topics --create --topic daily-data                     --bootstrap-server kafka:9092 --replication-factor 1 --partitions 5
      kafka-topics --create --topic 15min-data                     --bootstrap-server kafka:9092 --replication-factor 1 --partitions 5
      kafka-topics --create --topic options-data                   --bootstrap-server kafka:9092 --replication-factor 1 --partitions 5
      kafka-topics --create --topic historical-data                --bootstrap-server kafka:9092 --replication-factor 1 --partitions 5
      
      kafka-topics --create --topic processed-daily-data           --bootstrap-server kafka:9092 --replication-factor 1 --partitions 5
      kafka-topics --create --topic processed-15min-data           --bootstrap-server kafka:9092 --replication-factor 1 --partitions 5
      kafka-topics --create --topic processed-options-data         --bootstrap-server kafka:9092 --replication-factor 1 --partitions 5
      kafka-topics --create --topic processed-historical-data      --bootstrap-server kafka:9092 --replication-factor 1 --partitions 5
      echo 'Kafka topics created!'
      "
    networks:
      - stock_pipeline

      
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      - ZOOKEEPER_CLIENT_PORT=2181
      - ZOOKEEPER_TICK_TIME=2000
    ports:
      - "2181:2181"
    healthcheck:
      test: ["CMD-SHELL", "echo ruok | nc localhost 2181"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - stock_pipeline

  minio:
    image: minio/minio:latest
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    networks:
      - stock_pipeline

  prometheus:
    image: prom/prometheus:v2.54.1
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    networks:
      - stock_pipeline
    


  grafana:
    image: grafana/grafana:11.2.2
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
    networks:
      - stock_pipeline

  
  node-exporter:
    image: prom/node-exporter:latest
    container_name: node_exporter
    ports:
      - "9100:9100"
    networks:
      - stock_pipeline

volumes:
  mysql_data:
  influxdb_data:
  minio_data:
  grafana_data:
  postgres_data:
  postgres_backups:
  pgadmin_data:

networks:
  stock_pipeline:
    driver: bridge