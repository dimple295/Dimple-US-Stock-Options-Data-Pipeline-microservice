import os
import pandas as pd
import boto3
from influxdb_client import InfluxDBClient
from datetime import datetime, timedelta, timezone
import time
from requests.exceptions import HTTPError

class RealTimeDataFileWriter:
    def __init__(self):
        self.influx_url = os.getenv('INFLUXDB_URL')
        self.influx_token = os.getenv('INFLUXDB_TOKEN')
        self.influx_org = os.getenv('INFLUXDB_ORG')
        self.influx_bucket = os.getenv('INFLUXDB_BUCKET')

        self.aws_access_key = os.getenv('AWS_ACCESS_KEY_ID')
        self.aws_secret_key = os.getenv('AWS_SECRET_ACCESS_KEY')
        self.s3_bucket = os.getenv('S3_BUCKET_NAME')

        self.output_folder = './fifteenmin_data/'
        os.makedirs(self.output_folder, exist_ok=True)

        self.client = InfluxDBClient(url=self.influx_url, token=self.influx_token, org=self.influx_org)
        self.s3_client = boto3.client('s3',
                                      aws_access_key_id=self.aws_access_key,
                                      aws_secret_access_key=self.aws_secret_key)

    def export_fifteen_min_data(self):
        today = datetime.now(timezone.utc).date()
        yesterday = today - timedelta(days=1)

        # Option to override dates via environment (for Airflow or debugging)
        start_time = os.getenv('START_TIME') or datetime.combine(yesterday, datetime.min.time(), tzinfo=timezone.utc).isoformat()
        end_time = os.getenv('END_TIME') or datetime.combine(today, datetime.min.time(), tzinfo=timezone.utc).isoformat()
        #start_time = "2025-07-02T00:00:00Z"
        #end_time = "2025-07-03T00:00:00Z"


        print(f"Running InfluxDB query from {start_time} to {end_time}...")

        query = f'''
            from(bucket: "{self.influx_bucket}")
              |> range(start: {start_time}, stop: {end_time})
              |> filter(fn: (r) => r._measurement == "stock_15min")
              |> pivot(rowKey:["_time"], columnKey: ["_field"], valueColumn: "_value")
        '''

        for attempt in range(3):
            try:
                query_api = self.client.query_api()
                tables = query_api.query_data_frame(query, org=self.influx_org)

                if tables.empty:
                    print("No data retrieved for the specified range.")
                    return

                tables = tables.drop(columns=[col for col in tables.columns if col.startswith('result') or col.startswith('table')], errors='ignore')

                filename = f'stock_15min_{yesterday.strftime("%d%m%Y")}.csv'
                local_path = os.path.join(self.output_folder, filename)
                tables.to_csv(local_path, index=False)

                print(f"Saved CSV: {local_path} ({len(tables)} rows)")
                self.upload_to_s3(local_path, filename,'real_time_15min_data')
                break  # Success, no need to retry

            except HTTPError as http_err:
                print(f"HTTP error on attempt {attempt+1}: {http_err}")
                time.sleep(5)
            except Exception as e:
                print(f"General error on attempt {attempt+1}: {e}")
                time.sleep(5)

    def upload_to_s3(self, file_path, s3_key, folder_name):
        try:
            s3_full_key = f"{folder_name}/{s3_key}"
            self.s3_client.upload_file(file_path, self.s3_bucket, s3_full_key)
            print(f"Uploaded to S3: s3://{self.s3_bucket}/{s3_full_key}")
        except Exception as e:
            print(f"S3 upload failed for {s3_key}: {e}")

    def close_connection(self):
        self.client.close()
        print("InfluxDB connection closed.")


